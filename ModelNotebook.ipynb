{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakaplay/cnn_recogn/blob/main/ModelNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0FeHOtkmIOZ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlcpbpr6clg9",
        "outputId": "3a6eaef3-95a7-48bf-885c-385ad465722d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество кусочков 8x8: 190280\n",
            "Количество кусочков 16x16: 190280\n",
            "Всего входных кусочков: 190280\n",
            "Всего лейблов: 190280\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " up_sampling2d_3 (UpSamplin  (None, 16, 16, 3)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_transpose_8 (Conv2D  (None, 16, 16, 256)       7168      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_transpose_9 (Conv2D  (None, 16, 16, 768)       1770240   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_transpose_10 (Conv2  (None, 16, 16, 3)         20739     \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1798147 (6.86 MB)\n",
            "Trainable params: 1798147 (6.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "744/744 [==============================] - 67s 82ms/step - loss: 0.0433 - accuracy: 0.8326\n",
            "Epoch 2/10\n",
            "744/744 [==============================] - 56s 76ms/step - loss: 0.0359 - accuracy: 0.8483\n",
            "Epoch 3/10\n",
            "744/744 [==============================] - 56s 76ms/step - loss: 0.0347 - accuracy: 0.8524\n",
            "Epoch 4/10\n",
            "744/744 [==============================] - 56s 76ms/step - loss: 0.0339 - accuracy: 0.8552\n",
            "Epoch 5/10\n",
            "744/744 [==============================] - 56s 76ms/step - loss: 0.0335 - accuracy: 0.8572\n",
            "Epoch 6/10\n",
            "744/744 [==============================] - 56s 75ms/step - loss: 0.0331 - accuracy: 0.8586\n",
            "Epoch 7/10\n",
            "744/744 [==============================] - 56s 76ms/step - loss: 0.0329 - accuracy: 0.8596\n",
            "Epoch 8/10\n",
            "744/744 [==============================] - 56s 75ms/step - loss: 0.0326 - accuracy: 0.8611\n",
            "Epoch 9/10\n",
            "744/744 [==============================] - 56s 75ms/step - loss: 0.0325 - accuracy: 0.8616\n",
            "Epoch 10/10\n",
            "160/744 [=====>........................] - ETA: 43s - loss: 0.0322 - accuracy: 0.8634"
          ]
        }
      ],
      "source": [
        "def prepare_upscaling_data(image_paths, downscale_size=(8, 8), original_tile_size=(16, 16)):\n",
        "    downscale_images = []\n",
        "    original_tiles = []\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        # Вычисляем паддинг\n",
        "        width, height = image.size\n",
        "        padding_width = (original_tile_size[0] - width % original_tile_size[0]) % original_tile_size[0]\n",
        "        padding_height = (original_tile_size[1] - height % original_tile_size[1]) % original_tile_size[1]\n",
        "\n",
        "        # Добавляем паддинг\n",
        "        image = image.crop((-padding_width // 2, -padding_height // 2,\n",
        "                           width + padding_width - padding_width // 2,\n",
        "                           height + padding_height - padding_height // 2))\n",
        "\n",
        "        np_image = np.array(image)\n",
        "\n",
        "        for y in range(0, np_image.shape[0], original_tile_size[1]):\n",
        "            for x in range(0, np_image.shape[1], original_tile_size[0]):\n",
        "                original_tile = np_image[y:y+original_tile_size[1], x:x+original_tile_size[0]]\n",
        "                original_tiles.append(original_tile)\n",
        "                downscale_image = Image.fromarray(original_tile).resize(downscale_size)\n",
        "                downscale_images.append(np.array(downscale_image))\n",
        "\n",
        "    return np.array(downscale_images), np.array(original_tiles)\n",
        "\n",
        "image_paths = [f\"photos/({i}).jpg\" for i in range(12)]\n",
        "images, labels = prepare_upscaling_data(image_paths)\n",
        "\n",
        "print(f\"Количество кусочков 8x8: {len(images)}\")\n",
        "print(f\"Количество кусочков 16x16: {len(labels)}\")\n",
        "\n",
        "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
        "config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "img_size = (8, 8)\n",
        "output_size = (16, 16)\n",
        "\n",
        "def process_images_from_list(image_paths, input_size=(8, 8), label_size=(16, 16)):\n",
        "    input_chunks_all = []\n",
        "    label_chunks_all = []\n",
        "\n",
        "    input_chunks, label_chunks = prepare_upscaling_data(image_paths, input_size, label_size)\n",
        "\n",
        "    input_chunks_all.extend(input_chunks)\n",
        "    label_chunks_all.extend(label_chunks)\n",
        "\n",
        "    return input_chunks_all, label_chunks_all\n",
        "\n",
        "input_chunks_all, label_chunks_all = process_images_from_list(image_paths, img_size, output_size)\n",
        "\n",
        "print(\"Всего входных кусочков:\", len(input_chunks_all))\n",
        "print(\"Всего лейблов:\", len(label_chunks_all))\n",
        "\n",
        "input_chunks_all = np.array(input_chunks_all).astype('float32') / 255.0\n",
        "label_chunks_all = np.array(label_chunks_all).astype('float32') / 255.0\n",
        "\n",
        "input_chunks_all = input_chunks_all.reshape(-1, 8, 8, 3)\n",
        "label_chunks_all = label_chunks_all.reshape(-1, 16, 16, 3)\n",
        "\n",
        "def augment(chunk):\n",
        "    return chunk * np.random.uniform(0.5, 1, size=chunk.shape)\n",
        "\n",
        "input_chunks_all = np.array([augment(i) for i in input_chunks_all])\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input((8, 8, 3)),\n",
        "    tf.keras.layers.UpSampling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2DTranspose(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2DTranspose(768, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mae', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(input_chunks_all, label_chunks_all, epochs=10, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "fQrHVD7CxUbp",
        "outputId": "9e5bd2cf-811a-4a3c-a97a-0f3d4e2ecf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    355\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/saves.tar model.tar\n",
        "!tar -xf model.tar"
      ],
      "metadata": {
        "id": "ARp_9h6XxddD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwINRethgRpb"
      },
      "outputs": [],
      "source": [
        "model.save('upscale3.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "a = requests.get(\"https://nekofile.eu.org/5711d1b5863602eb4b1e4vifu\")\n",
        "file = open(\"img.jpg\", \"wb\")\n",
        "file.write(a.content)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "czu979BzxpCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8R_1gs1fpfa",
        "outputId": "e986c82b-c795-4892-9898-d0eec8ce3578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512/512 [==============================] - 1s 2ms/step\n",
            "Обработано 16384 из 55620 тайлов\n",
            "512/512 [==============================] - 1s 2ms/step\n",
            "Обработано 32768 из 55620 тайлов\n",
            "512/512 [==============================] - 1s 2ms/step\n",
            "Обработано 49152 из 55620 тайлов\n",
            "203/203 [==============================] - 1s 2ms/step\n",
            "Обработано 55620 из 55620 тайлов\n",
            "Апскейленное изображение сохранено в: imggg.png\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
        "config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Загрузка изображения из файла\n",
        "input_image_path = 'imgg.png'  # Замени на путь к твоему входному файлу\n",
        "output_image_path = 'imggg.png'  # Замени на путь для сохранения результата\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Загрузка модели\n",
        "model = tf.keras.models.load_model('upscale3.keras')\n",
        "\n",
        "\n",
        "\n",
        "def split_image_to_tiles(image_data, tile_size=(8, 8)):\n",
        "    \"\"\"\n",
        "    Разделяет изображение на кусочки заданного размера.\n",
        "    \"\"\"\n",
        "    tiles = []\n",
        "    height, width = image_data.shape[:2]\n",
        "\n",
        "    # Вычисляем новую ширину и высоту, кратные размеру тайла\n",
        "    new_width = ((width + tile_size[1] - 1) // tile_size[1]) * tile_size[1]\n",
        "    new_height = ((height + tile_size[0] - 1) // tile_size[0]) * tile_size[0]\n",
        "\n",
        "    # Создаем новое изображение с черным фоном\n",
        "    new_image = np.zeros((new_height, new_width, 3), dtype=np.uint8)\n",
        "\n",
        "    # Вычисляем координаты для вставки исходного изображения\n",
        "    x_offset = (new_width - width) // 2\n",
        "    y_offset = (new_height - height) // 2\n",
        "\n",
        "    # Вставляем исходное изображение в новое\n",
        "    new_image[y_offset:y_offset+height, x_offset:x_offset+width] = image_data\n",
        "\n",
        "    # Разделяем на тайлы\n",
        "    tiles = []\n",
        "    for y in range(0, new_height, tile_size[0]):\n",
        "        for x in range(0, new_width, tile_size[1]):\n",
        "            tile = new_image[y:y+tile_size[0], x:x+tile_size[1]]\n",
        "            tiles.append(tile)\n",
        "\n",
        "    return tiles, (new_height, new_width) # Возвращаем новый размер\n",
        "\n",
        "#def split_image_to_tiles(image_data, tile_size=(8, 8)):\n",
        "#    \"\"\"\n",
        "#    Разделяет изображение на кусочки заданного размера.\n",
        "#    \"\"\"\n",
        "#\n",
        "#    tiles = []\n",
        "#    height, width = image_data.shape[:2]\n",
        "#    print(height, width)\n",
        "#    height, width = (height // 8) * 8, (width // 8) * 8\n",
        "#    image_data = cv2.resize(image_data, (height, width), cv2.INTER_LINEAR)\n",
        "#    Image.fromarray(image_data).save('watafa.png', 'PNG')\n",
        "#    print(image_data.shape[:2])\n",
        "#    for y in range(0, height, tile_size[0]):\n",
        "#        for x in range(0, width, tile_size[1]):\n",
        "#            tile = image_data[y:min(y+tile_size[0], height), x:min(x+tile_size[1], width)]\n",
        "#            if tile.size > 0:\n",
        "#                tiles.append(tile)\n",
        "#    return tiles, (height, width)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def upscale_tiles(tiles, model, batch_size=16384):\n",
        "    \"\"\"\n",
        "    Применяет модель апскейлинга к каждому кусочку, обрабатывая по batch_size тайлов за раз.\n",
        "    \"\"\"\n",
        "    upscaled_tiles = []\n",
        "    total_tiles = len(tiles)\n",
        "    processed_tiles = 0\n",
        "\n",
        "    for i in range(0, total_tiles, batch_size):\n",
        "        batch_tiles = tiles[i: min(i + batch_size, total_tiles)]\n",
        "\n",
        "        # Преобразуем список тайлов в массив NumPy\n",
        "        batch_tiles = np.array(batch_tiles)\n",
        "\n",
        "        # Нормализация тайлов\n",
        "        batch_tiles = batch_tiles.astype('float32') / 255.0\n",
        "\n",
        "        # Предсказание модели\n",
        "        batch_upscaled_tiles = model.predict(batch_tiles)\n",
        "\n",
        "        # Преобразование обратно в формат изображения\n",
        "        batch_upscaled_tiles = (batch_upscaled_tiles * 255).astype(np.uint8)\n",
        "\n",
        "        # Добавляем обработанные тайлы в список\n",
        "        upscaled_tiles.extend(batch_upscaled_tiles)\n",
        "\n",
        "        processed_tiles += len(batch_tiles)\n",
        "        print(f\"Обработано {processed_tiles} из {total_tiles} тайлов\")\n",
        "\n",
        "    return upscaled_tiles\n",
        "\n",
        "\n",
        "def merge_tiles(tiles, original_image_size, tile_size=(16, 16)):  # Изменено tile_size\n",
        "    \"\"\"\n",
        "    Собирает апскейленные кусочки обратно в изображение.\n",
        "    \"\"\"\n",
        "    # Увеличиваем размер выходного изображения в два раза\n",
        "    upscaled_image = np.zeros((original_image_size[0] * 2, original_image_size[1] * 2, 3), dtype=np.uint8)\n",
        "    i = 0\n",
        "    for y in range(0, upscaled_image.shape[0], tile_size[0]):\n",
        "        for x in range(0, upscaled_image.shape[1], tile_size[1]):\n",
        "            if i < len(tiles):\n",
        "                # Изменяем размер тайла перед помещением в область изображения\n",
        "                upscaled_image[y:y+tile_size[0], x:x+tile_size[1]] = tiles[i][:tile_size[0], :tile_size[1]]\n",
        "                i += 1\n",
        "    return upscaled_image\n",
        "\n",
        "def bring_back_aspect_ratio(image, original_size):\n",
        "    \"\"\"\n",
        "    Изменяет размер изображения до исходных пропорций.\n",
        "    \"\"\"\n",
        "    original_height, original_width = original_size\n",
        "    resized_image = cv2.resize(image, (original_width * 2, original_height * 2), interpolation=cv2.INTER_CUBIC)\n",
        "    return resized_image\n",
        "\n",
        "\n",
        "# Загрузка изображения\n",
        "image = Image.open(input_image_path).convert('RGB') # Убираем альфа-канал\n",
        "original_image = np.array(image)\n",
        "\n",
        "# Обработка изображения\n",
        "tiles, new_size = split_image_to_tiles(original_image) # Получаем новый размер\n",
        "upscaled_tiles = upscale_tiles(tiles, model)\n",
        "upscaled_image = merge_tiles(upscaled_tiles, new_size) # Используем новый размер\n",
        "upscaled_image = bring_back_aspect_ratio(upscaled_image, original_image.shape[:2])\n",
        "\n",
        "# Сохранение апскейленного изображения\n",
        "upscaled_image_pil = Image.fromarray(upscaled_image)\n",
        "upscaled_image_pil.save(output_image_path)\n",
        "\n",
        "print(f\"Апскейленное изображение сохранено в: {output_image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"md.keras\")"
      ],
      "metadata": {
        "id": "bXnp85bL_tDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}